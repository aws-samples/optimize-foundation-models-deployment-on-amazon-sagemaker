{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy a Bitsandbytes and GPTQ quantized models on SageMaker with Hugging Face TGI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will deploy a pre-quantized Bitsandbytes 7 billion parameter [Llama 2 Chat model](https://huggingface.co/Trelis/Llama-2-7b-chat-hf-function-calling). , and then a pre-quantized GPTQ 7 billion parameter [Llama 2 Chat model](https://huggingface.co/Trelis/Llama-2-7b-chat-hf-function-calling-GPTQ)\n",
    "\n",
    "With Amazon SageMaker, you can deploy your machine learning (ML) models to make predictions, also known as inference. SageMaker provides a broad selection of ML infrastructure and model deployment options to help meet all your ML inference needs. It is a fully managed service and integrates with MLOps tools, so you can scale your model deployment, reduce inference costs, manage models more effectively in production, and reduce operational burden.\n",
    "For more details, please refer to the [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html).\n",
    "\n",
    "The original models is stored and served in half-precision fp16 format which translates to 2 bytes per parameter. Given that the model has 13 billion parameters, the model size translates 26GB which is too large to fit in the memory of a single A10 GPU which has only 24GB of memory. This requires us to use a more expensive multi-gpu instance such as a `ml.g5.12xlarge`. An alternative is to quantize the model which can significantly reduce the amount of VRAM required to host the model.  \n",
    "\n",
    "In this notebook, we will 1st deploy a 7 billion parameter model that has been pre-quantized using bitsandbytes to int8, thereby greatly reducing the memory footprint of the model from the initial FP16. See this [blog post](https://huggingface.co/blog/hf-bitsandbytes-integration) from Hugging Face for additional information \n",
    "\n",
    "Then we deploy the same model, but a one that has been pre-quantized to 4-bits using [GPTQ algorithm](https://arxiv.org/abs/2210.17323). With 4bit quantization the amount of memory per parameter is reduced from 2 bytes to 4 bits (0.5 bytes) which translates to a 75% reduction in memory footprint. This allows us to host the model on a single A10 GPU instance, such as a `ml.g5.xlarge`, which is significantly cheaper than a multi-gpu instance. As a disclaimer, quantization does result in a slight drop in model accuracy. However, the drop in accuracy is small and the model is still able to generate coherent responses, but it is important to evaluate the model on your use case.    \n",
    "\n",
    "*Llama 2 is licensed under the LLAMA 2 Community License, Copyright (c) Meta Platforms, Inc. All Rights Reserved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -Uq sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import json\n",
    "from sagemaker import Model\n",
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "boto3_session = boto3.session.Session()\n",
    "\n",
    "smr = boto3_session.client(\n",
    "    \"sagemaker-runtime\"\n",
    ")  # sagemaker runtime client for invoking the endpoint\n",
    "sm = boto3_session.client(\"sagemaker\")  # sagemaker client for creating the endpoint\n",
    "\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "\n",
    "sess = sagemaker.session.Session(\n",
    "    boto3_session, sagemaker_client=sm, sagemaker_runtime_client=smr\n",
    ")  # sagemaker session for interacting with different AWS APIs\n",
    "\n",
    "bucket = (\n",
    "    sess.default_bucket()\n",
    ")  # sagemaker session for interacting with different AWS APIs\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the Hugging Face Text Generation Inference (TGI) Container which runs the optimized [TGI](https://github.com/huggingface/text-generation-inference) LLM hosting solution from HuggingFace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# retrieve the llm image uri\n",
    "llm_image_uri = get_huggingface_llm_image_uri(\"huggingface\", version=\"1.0.3\", session=sess)\n",
    "llm_image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The helper function below will deploy the model to a SageMaker endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def deploy_model(\n",
    "    endpoint_name,\n",
    "    instance_type,\n",
    "    env=None,\n",
    "    image_uri=None,\n",
    "    model_artifact=None,\n",
    "    s3_bucket=None,\n",
    "    s3_prefix=None,\n",
    "    wait=True,\n",
    "):\n",
    "    \"\"\"Uploads the model artifact to S3 and deploys the model to SageMaker.\"\"\"\n",
    "    if model_artifact:\n",
    "        code_artifact = sess.upload_data(model_artifact, s3_bucket, s3_prefix)\n",
    "        print(f\"Inference Code tar ball uploaded to --- > {code_artifact}\")\n",
    "    else:\n",
    "        code_artifact = None\n",
    "\n",
    "    model = Model(\n",
    "        sagemaker_session=sess,\n",
    "        image_uri=image_uri,\n",
    "        model_data=code_artifact,\n",
    "        env=env,\n",
    "        role=role,\n",
    "    )\n",
    "\n",
    "    model.deploy(\n",
    "        initial_instance_count=1,\n",
    "        instance_type=instance_type,\n",
    "        endpoint_name=endpoint_name,\n",
    "        wait=wait,\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Llama 2 7B Chat quantized model with Bitsandbytes\n",
    "\n",
    "It will take around 7-8 minutes for the endpoint to be ready so please be patient.\n",
    "\n",
    "Note `HF_MODEL_QUANTIZE` parameter is set to `bitsandbytes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "llama_7b_bitsandbytes_endpoint_name = sagemaker.utils.name_from_base(\"llama2-7b-bitsandbytes\")\n",
    "\n",
    "llama_7b_bitsandbytes_env = {\n",
    "    \"HF_MODEL_ID\":\"Trelis/Llama-2-7b-chat-hf-function-calling\",\n",
    "    \"HF_TASK\":\"text-generation\",\n",
    "    \"SM_NUM_GPUS\": \"1\",\n",
    "    \"HF_MODEL_QUANTIZE\": \"bitsandbytes\"\n",
    "}\n",
    "\n",
    "llama_7b_bitsandbytes_model = deploy_model(\n",
    "    endpoint_name=llama_7b_bitsandbytes_endpoint_name,\n",
    "    instance_type=\"ml.g5.2xlarge\",\n",
    "    env=llama_7b_bitsandbytes_env,\n",
    "    image_uri=llm_image_uri,\n",
    "    wait=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the endpoint is deployed we can invoke it using the boto3 SDK. We will use the [Llama2 recommended prompt format](https://huggingface.co/blog/llama2#how-to-prompt-llama-2) along with a user instruction to create the final prompt which we will pass as a payload to the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_message = \"\"\"\\\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\"\n",
    "\n",
    "prompt = \"Why is New York City sometimes referred to as the Big Apple?\"\n",
    "prompt_template = f\"\"\"[INST] <<SYS>>{system_message}<</SYS>>\\n{prompt}[/INST]\"\"\"\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# invoke llm\n",
    "# first invocation is going to be slower. Subsequent ones should be faster\n",
    "\n",
    "body = {\n",
    "    \"inputs\": prompt_template,\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 250,\n",
    "        \"temperature\": 0.1,\n",
    "        \"return_full_text\": False, # if True this will return our original prompt along with the generated text\n",
    "    },\n",
    "}\n",
    "resp = smr.invoke_endpoint(\n",
    "    EndpointName=llama_7b_bitsandbytes_endpoint_name,\n",
    "    Body=json.dumps(body),\n",
    "    ContentType=\"application/json\",\n",
    ")\n",
    "output = json.loads(resp[\"Body\"].read().decode(\"utf-8\"))\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see it took around 10-12 seconds. This is since model takes time to load. Let's run the prompt again after the model is loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "resp = smr.invoke_endpoint(\n",
    "    EndpointName=llama_7b_bitsandbytes_endpoint_name,\n",
    "    Body=json.dumps(body),\n",
    "    ContentType=\"application/json\",\n",
    ")\n",
    "output = json.loads(resp[\"Body\"].read().decode(\"utf-8\"))\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see it takes almost half the time to get a response from Llama 2 7B Chat quantized model with Bitsandbytes when it is already loaded.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking Llama 2 7B Chat quantized model with Bitsandbytes\n",
    "\n",
    "https://huggingface.co/spaces/Xanthius/llama-token-counter\n",
    "\n",
    "- 147 input tokens.\n",
    "- 128 output tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "\n",
    "def benchmark_generation_speed(endpoint_name):\n",
    "    num_generated_tokens = 275\n",
    "    generation_time_list = []\n",
    "    num_generated_tokens_list = []\n",
    "\n",
    "    for i in tqdm(range(10)):\n",
    "        print(f\"Calling {endpoint_name} endpoint - #{i+1} time\")\n",
    "        start = time.time()\n",
    "        resp = smr.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            Body=json.dumps(body),\n",
    "            ContentType=\"application/json\",\n",
    "        )\n",
    "        end = time.time()\n",
    "        generation_time_list.append(end - start)\n",
    "        num_generated_tokens_list.append(275)\n",
    "\n",
    "    total_tokens = sum(num_generated_tokens_list)\n",
    "    total_seconds = sum(generation_time_list)\n",
    "    print(f\"Generated {total_tokens} tokens using {total_seconds} seconds, \"\n",
    "          f\"Generation speed: {total_tokens / total_seconds} tokens/s\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "benchmark_generation_speed(llama_7b_bitsandbytes_endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that for Llama 2 7B Chat quantized model with Bitsandbytes, generating 2750 tokens, took around 100 seconds, with 27 tokens per second."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below to delete the endpoint and avoid any additional charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clean up\n",
    "sm.delete_endpoint(EndpointName=llama_7b_bitsandbytes_endpoint_name)\n",
    "sm.delete_endpoint_config(EndpointConfigName=llama_7b_bitsandbytes_endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Llama 2 7B Chat quantized model with GPTQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will take around 7-8 minutes for the endpoint to be ready so please be patient. \n",
    "\n",
    "Note `HF_MODEL_QUANTIZE` parameter is set to `gptq`, as opposed to `bitsandbytes` we used previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "llama_7b_gptq_endpoint_name = sagemaker.utils.name_from_base(\"llama2-7b-gptq\")\n",
    "\n",
    "llama_7b_gptq_env = {\n",
    "    \"HF_MODEL_ID\": \"Trelis/Llama-2-7b-chat-hf-function-calling-GPTQ\",  # model_id from hf.co/models\n",
    "    \"HF_TASK\":\"text-generation\",\n",
    "    \"SM_NUM_GPUS\": \"1\",  # Number of GPU used per replica\n",
    "    \"HF_MODEL_QUANTIZE\": \"gptq\",  # serve a pre-quantized model,\n",
    "}\n",
    "\n",
    "llama_7b_gptq_model = deploy_model(\n",
    "    endpoint_name=llama_7b_gptq_endpoint_name,\n",
    "    instance_type=\"ml.g5.2xlarge\",\n",
    "    env=llama_7b_gptq_env,\n",
    "    image_uri=llm_image_uri,\n",
    "    wait=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the endpoint is deployed we will invoke it using the boto3 SDK again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# invoke llm\n",
    "# first invocation is going to be slower. Subsequent ones should be faster\n",
    "\n",
    "body = {\n",
    "    \"inputs\": prompt_template,\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 250,\n",
    "        \"temperature\": 0.1,\n",
    "        \"return_full_text\": False, # if True this will return our original prompt along with the generated text\n",
    "    },\n",
    "}\n",
    "resp = smr.invoke_endpoint(\n",
    "    EndpointName=llama_7b_gptq_endpoint_name,\n",
    "    Body=json.dumps(body),\n",
    "    ContentType=\"application/json\",\n",
    ")\n",
    "output = json.loads(resp[\"Body\"].read().decode(\"utf-8\"))\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see it took 11 seconds. This is since model takes time to load. Let's run the prompt again after the model is loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "resp = smr.invoke_endpoint(\n",
    "    EndpointName=llama_7b_gptq_endpoint_name,\n",
    "    Body=json.dumps(body),\n",
    "    ContentType=\"application/json\",\n",
    ")\n",
    "output = json.loads(resp[\"Body\"].read().decode(\"utf-8\"))\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see it takes almost half the time to get a response from Llama 2 7B Chat quantized model with GPTQ when it is already loaded.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking Llama 2 7B Chat quantized model with GPTQ\n",
    "\n",
    "https://huggingface.co/spaces/Xanthius/llama-token-counter\n",
    "\n",
    "- 147 input tokens.\n",
    "- 128 output tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "benchmark_generation_speed(llama_7b_gptq_endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that for Llama 2 7B Chat quantized model with GPTQ, generating 2750 tokens, took around 29 seconds, with 94 tokens per second."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below to delete the endpoint and avoid any additional charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clean up\n",
    "sm.delete_endpoint(EndpointName=llama_7b_gptq_endpoint_name)\n",
    "sm.delete_endpoint_config(EndpointConfigName=llama_7b_gptq_endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this notebook we deployed Llama 2 7B Chat quantized model Bitsandbytes and then with GPTQ. For each model we sent a prompt for inference. We saw that 1st request on a new endpoint took some time for warmup, but the 2nd invocation took less.  \n",
    "\n",
    "Lastly we compared the inference time between the Bitsandbytes and the GPTQ one, and saw that GPTQ took around 50% less time than the Bitsandbytes. \n",
    "We also saw that GPTQ quantized model was able to process x3.5 more tokens per second, comparing to Bitsandbytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
